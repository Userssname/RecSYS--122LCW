{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as pgo\n",
    "from sklearn import cluster\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 101)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = pd.read_csv('user_history.csv')\n",
    "history.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USER ID</th>\n",
       "      <th>nectar prague</th>\n",
       "      <th>joshua sleep</th>\n",
       "      <th>gallery fiesta</th>\n",
       "      <th>jerome jazz</th>\n",
       "      <th>motif polo</th>\n",
       "      <th>zigzag germany</th>\n",
       "      <th>susan sonar</th>\n",
       "      <th>alamo group</th>\n",
       "      <th>gilbert chicken</th>\n",
       "      <th>...</th>\n",
       "      <th>pandora oval</th>\n",
       "      <th>cabinet state</th>\n",
       "      <th>cecilia raja</th>\n",
       "      <th>memo darwin</th>\n",
       "      <th>enigma recycle</th>\n",
       "      <th>olivia image</th>\n",
       "      <th>world blonde</th>\n",
       "      <th>begin unit</th>\n",
       "      <th>rodeo santana</th>\n",
       "      <th>drum spring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>4500.000000</td>\n",
       "      <td>4500.000000</td>\n",
       "      <td>4500.000000</td>\n",
       "      <td>4500.000000</td>\n",
       "      <td>4500.000000</td>\n",
       "      <td>4500.000000</td>\n",
       "      <td>4500.000000</td>\n",
       "      <td>4500.000000</td>\n",
       "      <td>4500.000000</td>\n",
       "      <td>4500.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4500.000000</td>\n",
       "      <td>4500.000000</td>\n",
       "      <td>4500.000000</td>\n",
       "      <td>4500.000000</td>\n",
       "      <td>4500.000000</td>\n",
       "      <td>4500.000000</td>\n",
       "      <td>4500.000000</td>\n",
       "      <td>4500.000000</td>\n",
       "      <td>4500.000000</td>\n",
       "      <td>4500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>113376.351778</td>\n",
       "      <td>1.865722</td>\n",
       "      <td>1.638220</td>\n",
       "      <td>1.143836</td>\n",
       "      <td>0.985400</td>\n",
       "      <td>1.216889</td>\n",
       "      <td>2.272874</td>\n",
       "      <td>2.374980</td>\n",
       "      <td>1.043467</td>\n",
       "      <td>1.578136</td>\n",
       "      <td>...</td>\n",
       "      <td>1.627423</td>\n",
       "      <td>1.036980</td>\n",
       "      <td>1.841324</td>\n",
       "      <td>1.875481</td>\n",
       "      <td>1.132869</td>\n",
       "      <td>2.113052</td>\n",
       "      <td>1.509319</td>\n",
       "      <td>0.700393</td>\n",
       "      <td>0.824720</td>\n",
       "      <td>0.686132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>7184.873215</td>\n",
       "      <td>0.937386</td>\n",
       "      <td>1.301395</td>\n",
       "      <td>0.743591</td>\n",
       "      <td>0.712322</td>\n",
       "      <td>0.794407</td>\n",
       "      <td>2.134627</td>\n",
       "      <td>1.677591</td>\n",
       "      <td>0.717391</td>\n",
       "      <td>1.324045</td>\n",
       "      <td>...</td>\n",
       "      <td>1.128374</td>\n",
       "      <td>0.873155</td>\n",
       "      <td>1.144434</td>\n",
       "      <td>1.312701</td>\n",
       "      <td>0.697700</td>\n",
       "      <td>0.483498</td>\n",
       "      <td>0.805120</td>\n",
       "      <td>0.410686</td>\n",
       "      <td>0.502802</td>\n",
       "      <td>0.537353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>100953.000000</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001380</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.411463</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>107134.000000</td>\n",
       "      <td>1.096461</td>\n",
       "      <td>0.481934</td>\n",
       "      <td>0.466599</td>\n",
       "      <td>0.377794</td>\n",
       "      <td>0.562622</td>\n",
       "      <td>0.399109</td>\n",
       "      <td>0.806781</td>\n",
       "      <td>0.461352</td>\n",
       "      <td>0.504135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665749</td>\n",
       "      <td>0.280598</td>\n",
       "      <td>0.785005</td>\n",
       "      <td>0.418055</td>\n",
       "      <td>0.563602</td>\n",
       "      <td>1.787143</td>\n",
       "      <td>0.618802</td>\n",
       "      <td>0.328835</td>\n",
       "      <td>0.406434</td>\n",
       "      <td>0.249143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>113286.000000</td>\n",
       "      <td>1.956605</td>\n",
       "      <td>1.250749</td>\n",
       "      <td>1.101720</td>\n",
       "      <td>0.850522</td>\n",
       "      <td>1.107225</td>\n",
       "      <td>1.335345</td>\n",
       "      <td>2.102481</td>\n",
       "      <td>0.932965</td>\n",
       "      <td>1.002327</td>\n",
       "      <td>...</td>\n",
       "      <td>1.408540</td>\n",
       "      <td>0.706805</td>\n",
       "      <td>1.890321</td>\n",
       "      <td>1.934150</td>\n",
       "      <td>1.065851</td>\n",
       "      <td>2.116222</td>\n",
       "      <td>1.839018</td>\n",
       "      <td>0.734343</td>\n",
       "      <td>0.803667</td>\n",
       "      <td>0.548355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>119607.250000</td>\n",
       "      <td>2.587191</td>\n",
       "      <td>3.002011</td>\n",
       "      <td>1.780516</td>\n",
       "      <td>1.498610</td>\n",
       "      <td>1.811398</td>\n",
       "      <td>4.952487</td>\n",
       "      <td>4.044239</td>\n",
       "      <td>1.533543</td>\n",
       "      <td>2.959773</td>\n",
       "      <td>...</td>\n",
       "      <td>2.528821</td>\n",
       "      <td>2.012495</td>\n",
       "      <td>2.770093</td>\n",
       "      <td>3.163269</td>\n",
       "      <td>1.650293</td>\n",
       "      <td>2.443463</td>\n",
       "      <td>2.138760</td>\n",
       "      <td>1.017411</td>\n",
       "      <td>1.198217</td>\n",
       "      <td>1.049258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>125976.000000</td>\n",
       "      <td>4.521661</td>\n",
       "      <td>5.153343</td>\n",
       "      <td>3.125105</td>\n",
       "      <td>3.748970</td>\n",
       "      <td>4.089940</td>\n",
       "      <td>6.759219</td>\n",
       "      <td>6.565286</td>\n",
       "      <td>3.897725</td>\n",
       "      <td>5.177968</td>\n",
       "      <td>...</td>\n",
       "      <td>5.339951</td>\n",
       "      <td>3.002520</td>\n",
       "      <td>5.688518</td>\n",
       "      <td>4.505662</td>\n",
       "      <td>3.267691</td>\n",
       "      <td>3.696181</td>\n",
       "      <td>2.965128</td>\n",
       "      <td>1.895920</td>\n",
       "      <td>2.674754</td>\n",
       "      <td>2.818771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             USER ID  nectar prague  joshua sleep  gallery fiesta  \\\n",
       "count    4500.000000    4500.000000   4500.000000     4500.000000   \n",
       "mean   113376.351778       1.865722      1.638220        1.143836   \n",
       "std      7184.873215       0.937386      1.301395        0.743591   \n",
       "min    100953.000000       0.000558      0.000111        0.000072   \n",
       "25%    107134.000000       1.096461      0.481934        0.466599   \n",
       "50%    113286.000000       1.956605      1.250749        1.101720   \n",
       "75%    119607.250000       2.587191      3.002011        1.780516   \n",
       "max    125976.000000       4.521661      5.153343        3.125105   \n",
       "\n",
       "       jerome jazz   motif polo  zigzag germany  susan sonar  alamo group  \\\n",
       "count  4500.000000  4500.000000     4500.000000  4500.000000  4500.000000   \n",
       "mean      0.985400     1.216889        2.272874     2.374980     1.043467   \n",
       "std       0.712322     0.794407        2.134627     1.677591     0.717391   \n",
       "min       0.000407     0.000103        0.000347     0.000068     0.000327   \n",
       "25%       0.377794     0.562622        0.399109     0.806781     0.461352   \n",
       "50%       0.850522     1.107225        1.335345     2.102481     0.932965   \n",
       "75%       1.498610     1.811398        4.952487     4.044239     1.533543   \n",
       "max       3.748970     4.089940        6.759219     6.565286     3.897725   \n",
       "\n",
       "       gilbert chicken  ...  pandora oval  cabinet state  cecilia raja  \\\n",
       "count      4500.000000  ...   4500.000000    4500.000000   4500.000000   \n",
       "mean          1.578136  ...      1.627423       1.036980      1.841324   \n",
       "std           1.324045  ...      1.128374       0.873155      1.144434   \n",
       "min           0.000288  ...      0.001380       0.000017      0.000060   \n",
       "25%           0.504135  ...      0.665749       0.280598      0.785005   \n",
       "50%           1.002327  ...      1.408540       0.706805      1.890321   \n",
       "75%           2.959773  ...      2.528821       2.012495      2.770093   \n",
       "max           5.177968  ...      5.339951       3.002520      5.688518   \n",
       "\n",
       "       memo darwin  enigma recycle  olivia image  world blonde   begin unit  \\\n",
       "count  4500.000000     4500.000000   4500.000000   4500.000000  4500.000000   \n",
       "mean      1.875481        1.132869      2.113052      1.509319     0.700393   \n",
       "std       1.312701        0.697700      0.483498      0.805120     0.410686   \n",
       "min       0.000289        0.000358      0.411463      0.000562     0.000098   \n",
       "25%       0.418055        0.563602      1.787143      0.618802     0.328835   \n",
       "50%       1.934150        1.065851      2.116222      1.839018     0.734343   \n",
       "75%       3.163269        1.650293      2.443463      2.138760     1.017411   \n",
       "max       4.505662        3.267691      3.696181      2.965128     1.895920   \n",
       "\n",
       "       rodeo santana  drum spring  \n",
       "count    4500.000000  4500.000000  \n",
       "mean        0.824720     0.686132  \n",
       "std         0.502802     0.537353  \n",
       "min         0.000784     0.000001  \n",
       "25%         0.406434     0.249143  \n",
       "50%         0.803667     0.548355  \n",
       "75%         1.198217     1.049258  \n",
       "max         2.674754     2.818771  \n",
       "\n",
       "[8 rows x 101 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# elbow method\n",
    "sse = []\n",
    "list_k = list(range(1, 11))\n",
    "\n",
    "for k in list_k:\n",
    "    km = cluster.KMeans(n_clusters=k)\n",
    "    km.fit(history.drop(['USER ID'], axis=1)) # we need to drop the first column(user id)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "# Plot sse against k\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(list_k, sse, '-o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Sum of squared distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to elbow method, the best k should be 3\n",
    "kmeans = cluster.KMeans(3) \n",
    "kmeans.fit(history.drop(['USER ID'], axis=1))\n",
    "kmeans_y = kmeans.predict(history.drop(['USER ID'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate first 3000 users by their labels\n",
    "# Create three arrays for each cluster/group of users\n",
    "First_Group = []\n",
    "Second_Group = []\n",
    "Third_Group = []\n",
    "\n",
    "for i in range(3000): \n",
    "    if kmeans.labels_[i] == 0:\n",
    "        First_Group.append(history['USER ID'].iloc[i])\n",
    "    if kmeans.labels_[i] == 1:\n",
    "        Second_Group.append(history['USER ID'].iloc[i])\n",
    "    if kmeans.labels_[i] == 2:\n",
    "        Third_Group.append(history['USER ID'].iloc[i])\n",
    "        \n",
    "First_Group = np.asarray(First_Group)\n",
    "Second_Group = np.asarray(Second_Group)\n",
    "Third_Group = np.asarray(Third_Group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100965 100993 101016 101034 101049 101056 101062 101072 101090 101127\n",
      " 101133 101156 101179 101218 101227 101235 101237 101266 101297 101301\n",
      " 101303 101313 101335 101337 101363 101373 101377 101404 101467 101491\n",
      " 101509 101526 101536 101545 101589 101610 101644 101675 101695 101700\n",
      " 101708 101771 101792 101836 101850 101853 101859 101890 101930 101933\n",
      " 101934 101938 101948 101984 102003 102025 102055 102061 102072 102100\n",
      " 102108 102153 102155 102201 102229 102245 102253 102259 102282 102288\n",
      " 102297 102307 102320 102381 102390 102401 102419 102424 102425 102442\n",
      " 102474 102509 102516 102533 102538 102545 102659 102678 102694 102708\n",
      " 102718 102721 102763 102774 102783 102828 102879 102886 102902 102930\n",
      " 102942 102957 102967 102982 103002 103032 103071 103088 103107 103120\n",
      " 103135 103199 103209 103223 103224 103282 103301 103347 103349 103352\n",
      " 103407 103432 103433 103437 103445 103466 103470 103486 103502 103517\n",
      " 103527 103534 103543 103577 103589 103626 103641 103656 103674 103683\n",
      " 103714 103729 103732 103771 103772 103786 103787 103801 103809 103818\n",
      " 103831 103846 103860 103888 103889 103890 103891 103907 103921 103942\n",
      " 103944 103952 103968 103980 104007 104016 104038 104042 104058 104071\n",
      " 104086 104092 104116 104121 104137 104146 104154 104157 104183 104190\n",
      " 104212 104234 104337 104347 104351 104382 104396 104439 104469 104478\n",
      " 104487 104499 104512 104525 104547 104559 104574 104585 104590 104645\n",
      " 104652 104700 104714 104722 104733 104740 104747 104750 104752 104779\n",
      " 104809 104827 104836 104845 104854 104884 104897 104916 104945 104947\n",
      " 104950 104959 105015 105032 105044 105065 105068 105082 105098 105117\n",
      " 105129 105195 105205 105219 105257 105262 105331 105359 105369 105376\n",
      " 105385 105392 105425 105438 105486 105515 105525 105536 105542 105567\n",
      " 105598 105611 105621 105627 105662 105671 105681 105690 105727 105736\n",
      " 105754 105761 105768 105769 105781 105794 105800 105802 105840 105848\n",
      " 105869 105905 105913 105956 105969 105977 105994 106009 106016 106029\n",
      " 106037 106044 106051 106075 106085 106094 106112 106135 106143 106151\n",
      " 106157 106172 106191 106220 106229 106242 106253 106254 106256 106275\n",
      " 106304 106305 106314 106345 106361 106386 106423 106427 106435 106468\n",
      " 106477 106487 106496 106513 106515 106525 106534 106566 106567 106584\n",
      " 106591 106612 106622 106631 106642 106654 106747 106756 106761 106763\n",
      " 106785 106793 106830 106839 106852 106861 106865 106870 106892 106900\n",
      " 106937 106942 106943 106945 106951 106957 106973 106989 107014 107025\n",
      " 107040 107052 107057 107074 107080 107089 107091 107105 107154 107164\n",
      " 107200 107210 107217 107286 107321 107393 107396 107406 107417 107428\n",
      " 107500 107505 107514 107519 107593 107620 107633 107637 107653 107663\n",
      " 107664 107707 107712 107768 107776 107781 107786 107796 107810 107823\n",
      " 107830 107837 107851 107890 107926 107932 107935 107946 107955 107978\n",
      " 107984 107989 108006 108020 108050 108126 108133 108142 108170 108181\n",
      " 108186 108205 108227 108237 108248 108252 108258 108274 108296 108304\n",
      " 108306 108318 108357 108360 108366 108368 108380 108396 108419 108425\n",
      " 108440 108447 108486 108499 108506 108527 108533 108539 108550 108605\n",
      " 108647 108654 108684 108700 108735 108737 108745 108752 108803 108809\n",
      " 108819 108832 108842 108849 108860 108903 108912 108920 108929 108934\n",
      " 108959 108983 108993 109010 109043 109047 109048 109057 109058 109067\n",
      " 109080 109094 109112 109122 109130 109145 109149 109159 109169 109174\n",
      " 109178 109249 109265 109287 109317 109347 109380 109419 109426 109439\n",
      " 109452 109460 109487 109498 109517 109534 109544 109552 109555 109583\n",
      " 109615 109680 109703 109712 109728 109730 109744 109747 109767 109773\n",
      " 109806 109812 109816 109835 109862 109911 109934 109952 109957 109985\n",
      " 109999 110007 110012 110019 110032 110035 110047 110063 110071 110080\n",
      " 110087 110101 110108 110130 110139 110153 110180 110234 110244 110256\n",
      " 110268 110275 110282 110293 110314 110325 110338 110362 110365 110367\n",
      " 110368 110380 110416 110425 110453 110457 110464 110475 110496 110565\n",
      " 110596 110619 110626 110645 110662 110665 110674 110679 110687 110713\n",
      " 110718 110721 110746 110769 110778 110804 110816 110851 110879 110886\n",
      " 110893 110909 110915 110935 110940 110946 110963 110983 110992 111002\n",
      " 111023 111030 111061 111102 111112 111138 111168 111171 111173 111190\n",
      " 111217 111228 111276 111282 111290 111302 111319 111355 111374 111382\n",
      " 111391 111440 111459 111476 111479 111508 111515 111517 111526 111545\n",
      " 111549 111570 111573 111586 111597 111609 111618 111630 111669 111697\n",
      " 111714 111720 111748 111757 111766 111772 111779 111783 111791 111792\n",
      " 111827 111840 111845 111882 111886 111889 111896 111901 111915 111938\n",
      " 111946 111971 111998 112013 112015 112020 112039 112133 112157 112200\n",
      " 112221 112266 112276 112292 112314 112328 112339 112361 112367 112374\n",
      " 112386 112412 112431 112462 112478 112493 112498 112515 112528 112529\n",
      " 112543 112550 112562 112568 112606 112626 112638 112660 112670 112723\n",
      " 112729 112741 112752 112773 112782 112845 112866 112874 112889 112894\n",
      " 112945 112955 112960 112972 112979 112985 112997 113009 113015 113032\n",
      " 113057 113060 113088 113097 113107 113139 113148 113167 113202 113205\n",
      " 113206 113231 113237 113262 113271 113287 113291 113293 113310 113320\n",
      " 113331 113350 113358 113373 113383 113401 113422 113432 113467 113476\n",
      " 113508 113529 113530 113550 113557 113563 113571 113593 113605 113618\n",
      " 113645 113656 113661 113662 113682 113704 113712 113721 113726 113735\n",
      " 113744 113753 113791 113801 113864 113865 113870 113889 113898 113902\n",
      " 113917 113925 113930 113963 113968 113994 114005 114014 114024 114036\n",
      " 114045 114051 114058 114075 114102 114109 114115 114126 114132 114136\n",
      " 114138 114177 114190 114280 114292 114314 114316 114318 114324 114333\n",
      " 114346 114357 114379 114457 114461 114476 114492 114507 114508 114520\n",
      " 114525 114532 114541 114554 114565 114590 114603 114615 114631 114634\n",
      " 114649 114669 114714 114732 114740 114745 114787 114792 114821 114823\n",
      " 114830 114869 114936 114943 114963 114970 114980 114985 114992 114995\n",
      " 115003 115015 115017 115045 115066 115099 115121 115122 115138 115155\n",
      " 115168 115225 115250 115277 115305 115341 115353 115363 115369 115415\n",
      " 115423 115437 115438 115454 115464 115472 115488 115496 115499 115528\n",
      " 115535 115548 115561 115565 115574 115629 115680 115694 115696 115722\n",
      " 115747 115754 115763 115769 115778 115786 115813 115836 115874 115888\n",
      " 115912 115920 115930 115933 115938 115967 115973 115997 116000 116016\n",
      " 116020 116028 116096 116097 116112 116120 116153 116163 116165 116170\n",
      " 116172 116206 116208 116209 116217 116238 116269 116338 116341 116351\n",
      " 116362 116426 116445 116484 116516 116520 116525 116536 116539 116553\n",
      " 116621 116625 116631 116644 116680 116688 116693 116703 116720 116744\n",
      " 116754 116762 116764 116772 116785 116808 116818 116829 116854 116873\n",
      " 116924 116950 116975 117019 117094 117101 117117 117118 117168 117232\n",
      " 117236 117243 117259 117273 117282 117294 117297 117305 117314 117324\n",
      " 117344 117354 117370 117383 117393 117433 117447 117478 117486 117491\n",
      " 117502 117507 117512]\n"
     ]
    }
   ],
   "source": [
    "print(First_Group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "983"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(First_Group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1009"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Second_Group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1008"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Third_Group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#--------------------PART2---------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 101)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reformat = pd.read_csv('user_data.csv')\n",
    "reformat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150062, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv('user_ratings.csv')\n",
    "ratings.shape\n",
    "#history.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#According to 3 clusterings, split each USER_ID array into train/test arrays\n",
    "First_Group_train, First_Group_test = train_test_split(First_Group, test_size=0.2)\n",
    "Second_Group_train, Second_Group_test = train_test_split(Second_Group, test_size=0.2)\n",
    "Third_Group_train, Third_Group_test = train_test_split(Third_Group, test_size=0.2)\n",
    "\n",
    "#Sort again those arrays in ascending order\n",
    "First_Group_train.sort()\n",
    "First_Group_test.sort()\n",
    "Second_Group_train.sort()\n",
    "Second_Group_test.sort()\n",
    "Third_Group_train.sort()\n",
    "Third_Group_test.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Three Dataframes corresponding to train/test arrays\n",
    "Clstr1_train = reformat[reformat['USER_ID'].isin(First_Group_train)]\n",
    "Clstr1_test = reformat[reformat['USER_ID'].isin(First_Group_test)]\n",
    "Clstr2_train = reformat[reformat['USER_ID'].isin(Second_Group_train)]\n",
    "Clstr2_test = reformat[reformat['USER_ID'].isin(Second_Group_test)]\n",
    "Clstr3_train = reformat[reformat['USER_ID'].isin(Third_Group_train)]\n",
    "Clstr3_test = reformat[reformat['USER_ID'].isin(Third_Group_test)]\n",
    "\n",
    "#print(Clstr3_test)\n",
    "\n",
    "#print(Third_Group_train)\n",
    "#len(Third_Group_train)\n",
    "#type(Third_Group_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Since the target products in each dataframes are the same: exacly as those 100 products,\n",
    "##So we store those products into a list\n",
    "target_products = list(reformat.columns) \n",
    "target_products.remove(target_products[0]) #remove the item 'USER_ID'\n",
    "target_products = np.asarray(target_products)\n",
    "\n",
    "#type(target_products)\n",
    "#print(target_products)\n",
    "#len(target_products)\n",
    "##After rechecking, we know that array is what we need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rating_clusters(dataframe):\n",
    "    users = []\n",
    "    products = []\n",
    "    ratings = []\n",
    "    temp1 = dataframe.iloc[:,1:101]\n",
    "    temp2 = np.array(temp1)\n",
    "    rownumber = len(temp2)\n",
    "    colnumber = temp2.shape[1]\n",
    "    #Get ndArray of all column names \n",
    "    columnNames = list(dataframe.columns) \n",
    "    columnNames.remove(columnNames[0]) #remove the item 'USER_ID'\n",
    "    columnNames = np.asarray(columnNames)\n",
    "    #Get ndArray of all column names \n",
    "    rowNames = dataframe.iloc[:, 0]\n",
    "    rowNames = np.array(rowNames)\n",
    "    \n",
    "    for i in range(rownumber): \n",
    "        for j in range(colnumber):\n",
    "            users.append(rowNames[i])\n",
    "            products.append(columnNames[j])\n",
    "            ratings.append(temp2[i,j])\n",
    "    #reformat3_ratings = np.array(reformat3_ratings)\n",
    "    users = np.array(users)\n",
    "    products = np.array(products)\n",
    "    ratings = np.array(ratings)\n",
    "    return users,products,ratings\n",
    "\n",
    "users1_train,products1_train,ratings1_train = load_rating_clusters(Clstr1_train)\n",
    "users1_test,products1_test,ratings1_test = load_rating_clusters(Clstr1_test)\n",
    "users2_train,products2_train,ratings2_train = load_rating_clusters(Clstr2_train)\n",
    "users2_test,products2_test,ratings2_test = load_rating_clusters(Clstr2_test)\n",
    "users3_train,products3_train,ratings3_train = load_rating_clusters(Clstr3_train)\n",
    "users3_test,products3_test,ratings3_test = load_rating_clusters(Clstr3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100965 100965 100965 ... 117512 117512 117512] 78600\n"
     ]
    }
   ],
   "source": [
    "for i in range (len(users1_train)):\n",
    "    G1user_train = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 105662 is out of bounds for axis 0 with size 983",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-e6b02af86c24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0msgd_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_movies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mtrain_sgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msgd_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-e6b02af86c24>\u001b[0m in \u001b[0;36mtrain_sgd\u001b[0;34m(model, epochs)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrating\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffled_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffled_movies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffled_ratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m# update the model using the gradient at a single example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0msingle_example_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;31m# after each Epoch, we'll evaluate our model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-e6b02af86c24>\u001b[0m in \u001b[0;36msingle_example_step\u001b[0;34m(model, user, movie, rating)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;34m\"\"\"Update the model using the gradient at a single training example\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0muser_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmovie\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mgrad_users\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmovie_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmovie\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# the gradient for the user_features matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mgrad_movies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0muser_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# the gradient for the movie_features matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 105662 is out of bounds for axis 0 with size 983"
     ]
    }
   ],
   "source": [
    "##BYJW3\n",
    "train_ratings = ratings1_train\n",
    "\n",
    "learning_rate = 0.005\n",
    "k = 10 # the number of features (for each user/movie)\n",
    "m = len(train_ratings) # the size of the training set\n",
    "n_users = len(First_Group)\n",
    "n_movies = 100\n",
    "\n",
    "train_users = users1_train\n",
    "train_movies = products1_train\n",
    "\n",
    "def initialize(n_users, n_movies, k):\n",
    "    \"\"\"Initalize a random model, and normalize it so that it has sensible mean and variance\"\"\"\n",
    "    # (The normalization helps make sure we start out at a reasonable parameter scale, which speeds up training)\n",
    "    user_features = np.random.normal(size=(n_users, k))\n",
    "    movie_features = np.random.normal(size=(n_movies, k))\n",
    "    raw_predictions = predict((user_features, movie_features))\n",
    "    \n",
    "    s = np.sqrt(2*raw_predictions.std()) # We want to start out with roughly unit variance\n",
    "    b = np.sqrt((3.5 - raw_predictions.mean()/s)/k) #We want to start out with average rating 3.5\n",
    "    user_features /= s\n",
    "    user_features += b\n",
    "    movie_features /= s\n",
    "    movie_features += b\n",
    "    \n",
    "    return (user_features, movie_features)\n",
    "\n",
    "def predict(model):\n",
    "    \"\"\"The model's predictions for all user/movie pairs\"\"\"\n",
    "    user_features, movie_features = model\n",
    "    return user_features @ movie_features.T\n",
    "\n",
    "def single_example_step(model, user, movie, rating):\n",
    "    \"\"\"Update the model using the gradient at a single training example\"\"\"\n",
    "    user_features, movie_features = model\n",
    "    residual = np.dot(user_features[user], movie_features[movie]) - rating\n",
    "    grad_users = 2 * residual * movie_features[movie] # the gradient for the user_features matrix\n",
    "    grad_movies = 2 * residual * user_features[user] # the gradient for the movie_features matrix\n",
    "    user_features[user] -= learning_rate*grad_users\n",
    "    movie_features[movie] -= learning_rate*grad_movies\n",
    "\n",
    "def train_sgd(model, epochs):\n",
    "    \"\"\"Train the model for a number of epochs via SGD (batch size=1)\"\"\"\n",
    "    user_features, movie_features = model\n",
    "    # It's good practice to shuffle your data before doing batch gradient descent,\n",
    "    # so that each mini-batch peforms like a random sample from the dataset\n",
    "    shuffle = np.random.permutation(m) \n",
    "    shuffled_users = train_users[shuffle]\n",
    "    shuffled_movies = train_movies[shuffle]\n",
    "    shuffled_ratings = train_ratings[shuffle]\n",
    "    #print(train_users,len(shuffle))\n",
    "    #print(type(train_users),type(shuffle))\n",
    "    #print(shuffle, len(shuffle))\n",
    "    #print(shuffled_ratings, len(shuffled_ratings))\n",
    "    for epoch in range(epochs):\n",
    "        for user, movie, rating in zip(shuffled_users, shuffled_movies, shuffled_ratings):\n",
    "            # update the model using the gradient at a single example\n",
    "            single_example_step(model, user, movie, rating)\n",
    "        # after each Epoch, we'll evaluate our model\n",
    "        predicted = predict(model)\n",
    "        train_loss = np.mean((train_ratings - predicted[train_users, train_movies])**2)\n",
    "        test_loss = np.mean((test_ratings - predicted[test_users, test_movies])**2)\n",
    "        print(\"Loss after epoch #{} is: train/{} --- test/{}\".format(epoch+1, train_loss, test_loss))\n",
    "    \n",
    "sgd_model = initialize(n_users, n_movies, k)\n",
    "train_sgd(sgd_model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "\n",
    "def initialize(n_users, n_products):\n",
    "    \"\"\"Initalize a random model, and normalize it so that it has sensible mean and variance\"\"\"\n",
    "    # (The normalization helps make sure we start out at a reasonable parameter scale, which speeds up training)\n",
    "    user_features = np.random.normal(size=(n_users,10)) # still setting the number of features as 10\n",
    "    product_features = np.random.normal(size=(n_products,10)) # still setting the number of features as 10\n",
    "    raw_predictions = predict((user_features, product_features))\n",
    "    \n",
    "    s = np.sqrt(2*raw_predictions.std()) # We want to start out with roughly unit variance\n",
    "    b = np.sqrt(5 - raw_predictions.mean()/s) #We want to start out with average rating 5\n",
    "    user_features /= s\n",
    "    user_features += b\n",
    "    product_features /= s\n",
    "    product_features += b\n",
    "    \n",
    "    return (user_features, product_features)\n",
    "\n",
    "def predict(model):\n",
    "    \"\"\"The model's predictions for all user/product pairs\"\"\"\n",
    "    user_features, product_features = model\n",
    "    return user_features @ product_features.T\n",
    "\n",
    "sgd_model_1 = initialize(len(First_Group), len(target_products))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def single_example_step(model, user, product, rating, train_users, train_products):\n",
    "    \"\"\"Update the model using the gradient at a single training example\"\"\"\n",
    "    user_features, product_features = model\n",
    "    residual = np.dot(user_features[np.argwhere(train_users==user)], product_features[np.argwhere(train_products==product)]) #- rating\n",
    "    for i in range(len(residual)):\n",
    "        residual[i]-=rating[i]\n",
    "    grad_users = 2 * residual * product_features[np.argwhere(train_products==product)] # the gradient for the user_features matrix\n",
    "    grad_products = 2 * residual * user_features[np.argwhere(train_users==user)] # the gradient for the product_features matrix\n",
    "    #print(grad_users.shape,grad_products.shape)\n",
    "    user_features[np.argwhere(train_users==user)] -= learning_rate*grad_users\n",
    "    product_features[np.argwhere(train_products==product)] -= learning_rate*grad_products\n",
    "\n",
    "def train_sgd(model, epochs, train_users, train_products, train_ratings, test_users, test_products, test_ratings):\n",
    "    \"\"\"Train the model for a number of epochs via SGD (batch size=1)\"\"\"\n",
    "    user_features, movie_features = model\n",
    "    # It's good practice to shuffle your data before doing batch gradient descent,\n",
    "    # so that each mini-batch peforms like a random sample from the dataset\n",
    "    shuffle = np.random.permutation(len(train_ratings))\n",
    "    print(len(shuffle), len(train_users))\n",
    "    \"\"\"\n",
    "    shuffled_users = train_users[shuffle]\n",
    "    shuffled_products = train_products[shuffle]\n",
    "    shuffled_ratings = train_ratings[shuffle]\n",
    "    for epoch in range(epochs):\n",
    "        for user, product, rating in zip(shuffled_users, shuffled_products, shuffled_ratings):\n",
    "            # update the model using the gradient at a single example\n",
    "            single_example_step(model, user, product, rating, train_users, train_products)\n",
    "        # after each Epoch, we'll evaluate our model\n",
    "        \n",
    "        predicted = predict(model)\n",
    "        train_loss = np.mean((train_ratings - predicted[train_users, train_products])**2)\n",
    "        test_loss = np.mean((test_ratings - predicted[test_users, test_products])**2)\n",
    "        print(\"Loss after epoch #{} is: train/{} --- test/{}\".format(epoch+1, train_loss, test_loss))\n",
    "     \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_model_1 = initialize(len(users1_train), len(products1_train))\n",
    "#train_sgd(sgd_model_1, 10, users1_train, products1_train, ratings1_train, users1_test, products1_test, ratings1_test)\n",
    "#sgd_model_2 = initialize(len(users2_train), len(products2_train))\n",
    "#train_sgd(sgd_model_2, 10, users2_train, products2_train, ratings2_train, users2_test, products2_test, ratings2_test)\n",
    "#sgd_model_3 = initialize(len(users3_train), len(products3_train))\n",
    "#train_sgd(sgd_model_3, 10, users3_train, products3_train, ratings3_train, users3_test, products3_test, ratings3_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n",
      "(983, 1) (786, 1)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-f6bb31cc4217>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8.\u001b[0m \u001b[1;31m# Since we are averaging very sparse gradients,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# the gradients will be small and we can use a large learning rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mtrain_full\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_model_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFirst_Group\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp1_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr1_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# We only get a single update to the model from each epoch, so we'll need a lot more epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[0mfull_model_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSecond_Group\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp2_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8.\u001b[0m \u001b[1;31m# Since we are averaging very sparse gradients,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-f6bb31cc4217>\u001b[0m in \u001b[0;36mtrain_full\u001b[1;34m(model, epochs, train_users, train_products, train_ratings)\u001b[0m\n\u001b[0;32m     30\u001b[0m        \u001b[0mall_examples_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_users\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_products\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_ratings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m        \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m        \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ratings\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_users\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_products\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m        \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_ratings\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_users\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_products\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m        \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loss after epoch #{} is: train/{} --- test/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    " def all_examples_step(model, train_users, train_products, train_ratings):\n",
    "    \"\"\"Update the model using the gradient averaged over all training examples\"\"\"\n",
    "    user_features, product_features = model\n",
    "    # To average the gradient over all training examples, it's convenient to\n",
    "    #    initialize arrays of zeros to hold the full gradients, and then update\n",
    "    #    these arrays at each training example, just like in the SGD procedure\n",
    "    grad_users = np.zeros_like(user_features)\n",
    "    grad_products = np.zeros_like(product_features)\n",
    "    # We only need to compute the model's predicted ratings once\n",
    "    predicted = predict(model)\n",
    "    for user, product, rating in zip(train_users, train_products, train_ratings):\n",
    "        # Mimic the SGD procedure, but store the gradients so they can be averaged\n",
    "        residual = predicted[np.argwhere(train_users==user), np.argwhere(train_products==product)] - rating\n",
    "        #for i in range(len(residual)):\n",
    "        #   residual[i]-=rating[i]\n",
    "        #print(grad_users.shape,grad_products.shape)\n",
    "        \n",
    "        grad_users[np.argwhere(train_users==user)] += 2 * residual * product_features[np.argwhere(train_products==product)]\n",
    "        grad_products[np.argwhere(train_products==product)] += 2 * residual * user_features[np.argwhere(train_users==user)]\n",
    "        \n",
    "    user_features -= learning_rate/(len(train_ratings)) * grad_users # Update using the averaged gradients\n",
    "    product_features -= learning_rate/(len(train_ratings)) * grad_products\n",
    "    \n",
    "\n",
    "    \n",
    "def train_full(model, epochs, train_users, train_products, train_ratings):\n",
    "    \"\"\"Train the model for a number of epochs using gradients estimated from the entire training set\"\"\"\n",
    "    user_features, product_features = model\n",
    "    for epoch in range(epochs):\n",
    "        all_examples_step(model, train_users, train_products, train_ratings)\n",
    "        predicted = predict(model)\n",
    "        train_loss = np.mean((train_ratings - predicted[train_users, train_products])**2)\n",
    "        test_loss = np.mean((test_ratings - predicted[test_users, test_products])**2)\n",
    "        print(\"Loss after epoch #{} is: train/{} --- test/{}\".format(epoch+1, train_loss, test_loss))\n",
    "        \n",
    "full_model_1 = initialize(len(First_Group), len(p1_train))\n",
    "learning_rate = 8. # Since we are averaging very sparse gradients,\n",
    "# the gradients will be small and we can use a large learning rate\n",
    "train_full(full_model_1, 100, First_Group, p1_train, r1_train) # We only get a single update to the model from each epoch, so we'll need a lot more epochs\n",
    "full_model_2 = initialize(len(Second_Group), len(p2_train))\n",
    "learning_rate = 8. # Since we are averaging very sparse gradients,\n",
    "# the gradients will be small and we can use a large learning rate\n",
    "train_full(full_model_2, 100, Second_Group, p2_train, r2_train) # We only get a single update to the model from each epoch, so we'll need a lot more epochs\n",
    "full_model_3 = initialize(len(Third_Group), len(p3_train))\n",
    "learning_rate = 8. # Since we are averaging very sparse gradients,\n",
    "# the gradients will be small and we can use a large learning rate\n",
    "train_full(full_model_3, 100, Third_Group, p3_train, r3_train) # We only get a single update to the model from each epoch, so we'll need a lot more epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##size preparations for training\n",
    "m1_train = len(ratings1_train) # the size of the training set of 1st clustering \n",
    "n1_users_train = max(First_Group_train)+1 # the largest index, plus 1\n",
    "n_products = len(target_products)+1  #since we need to computer max(train_movies)+1 \n",
    "\n",
    "##temprary we set:\n",
    "learning_rate = 0.005\n",
    "k = 10\n",
    "\n",
    "def predict(model):\n",
    "    \"\"\"The model's predictions for all user/movie pairs\"\"\"\n",
    "    user_features, movie_features = model\n",
    "    return user_features @ movie_features.T\n",
    "\n",
    "##define initializae function\n",
    "def initialize(n_users, n_movies, k):\n",
    "    \"\"\"Initalize a random model, and normalize it so that it has sensible mean and variance\"\"\"\n",
    "    # (The normalization helps make sure we start out at a reasonable parameter scale, which speeds up training)\n",
    "    user_features = np.random.normal(size=(n_users, k))\n",
    "    movie_features = np.random.normal(size=(n_movies, k))\n",
    "    raw_predictions = predict((user_features, movie_features))\n",
    "    \n",
    "    s = np.sqrt(2*raw_predictions.std()) # We want to start out with roughly unit variance\n",
    "    b = np.sqrt((3.5 - raw_predictions.mean()/s)/k) #We want to start out with average rating 3.5\n",
    "    user_features /= s\n",
    "    user_features += b\n",
    "    movie_features /= s\n",
    "    movie_features += b\n",
    "    \n",
    "    return (user_features, movie_features)\n",
    "\n",
    "sgd_model_Group1_train = initialize(n1_users_train, n_products, k)\n",
    "#(sgd_model_Group1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def single_example_step(model, user, movie, rating):\n",
    "    \"\"\"Update the model using the gradient at a single training example\"\"\"\n",
    "    user_features, movie_features = model\n",
    "    residual = np.dot(user_features[user], movie_features[movie]) - rating\n",
    "    grad_users = 2 * residual * movie_features[movie] # the gradient for the user_features matrix\n",
    "    grad_movies = 2 * residual * user_features[user] # the gradient for the movie_features matrix\n",
    "    user_features[user] -= learning_rate*grad_users\n",
    "    movie_features[movie] -= learning_rate*grad_movies\n",
    "\n",
    "def train_sgd(model, epochs):\n",
    "    \"\"\"Train the model for a number of epochs via SGD (batch size=1)\"\"\"\n",
    "    user_features, movie_features = model\n",
    "    # It's good practice to shuffle your data before doing batch gradient descent,\n",
    "    # so that each mini-batch peforms like a random sample from the dataset\n",
    "    shuffle = np.random.permutation(m1_train)     ##!!!notice the vari\n",
    "    shuffled_users = train_users[shuffle]\n",
    "    shuffled_movies = train_movies[shuffle]\n",
    "    shuffled_ratings = train_ratings[shuffle]\n",
    "    for epoch in range(epochs):\n",
    "        for user, movie, rating in zip(shuffled_users, shuffled_movies, shuffled_ratings):\n",
    "            # update the model using the gradient at a single example\n",
    "            single_example_step(model, user, movie, rating)\n",
    "        # after each Epoch, we'll evaluate our model\n",
    "        predicted = predict(model)\n",
    "        train_loss = np.mean((train_ratings - predicted[train_users, train_movies])**2)\n",
    "        test_loss = np.mean((test_ratings - predicted[test_users, test_movies])**2)\n",
    "        print(\"Loss after epoch #{} is: train/{} --- test/{}\".format(epoch+1, train_loss, test_loss))\n",
    "\n",
    "train_sgd(sgd_model_Group1_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning_rate = 0.005\n",
    "#k = 10 # the number of features (for each user/movie)\n",
    "m = len(ratings1_train) # the size of the training set\n",
    "n_users = max(First_Group_train)+1 # the largest index, plus 1\n",
    "n_movies = max(train_movies)+1\n",
    "\n",
    "def initialize(n_users, n_movies, k):\n",
    "    \"\"\"Initalize a random model, and normalize it so that it has sensible mean and variance\"\"\"\n",
    "    # (The normalization helps make sure we start out at a reasonable parameter scale, which speeds up training)\n",
    "    user_features = np.random.normal(size=(n_users, k))\n",
    "    movie_features = np.random.normal(size=(n_movies, k))\n",
    "    raw_predictions = predict((user_features, movie_features))\n",
    "    \n",
    "    s = np.sqrt(2*raw_predictions.std()) # We want to start out with roughly unit variance\n",
    "    b = np.sqrt((3.5 - raw_predictions.mean()/s)/k) #We want to start out with average rating 3.5\n",
    "    user_features /= s\n",
    "    user_features += b\n",
    "    movie_features /= s\n",
    "    movie_features += b\n",
    "    \n",
    "    return (user_features, movie_features)\n",
    "\n",
    "def predict(model):\n",
    "    \"\"\"The model's predictions for all user/movie pairs\"\"\"\n",
    "    user_features, movie_features = model\n",
    "    return user_features @ movie_features.T\n",
    "\n",
    "def single_example_step(model, user, movie, rating):\n",
    "    \"\"\"Update the model using the gradient at a single training example\"\"\"\n",
    "    user_features, movie_features = model\n",
    "    residual = np.dot(user_features[user], movie_features[movie]) - rating\n",
    "    grad_users = 2 * residual * movie_features[movie] # the gradient for the user_features matrix\n",
    "    grad_movies = 2 * residual * user_features[user] # the gradient for the movie_features matrix\n",
    "    user_features[user] -= learning_rate*grad_users\n",
    "    movie_features[movie] -= learning_rate*grad_movies\n",
    "\n",
    "def train_sgd(model, epochs):\n",
    "    \"\"\"Train the model for a number of epochs via SGD (batch size=1)\"\"\"\n",
    "    user_features, movie_features = model\n",
    "    # It's good practice to shuffle your data before doing batch gradient descent,\n",
    "    # so that each mini-batch peforms like a random sample from the dataset\n",
    "    shuffle = np.random.permutation(m) \n",
    "    shuffled_users = train_users[shuffle]\n",
    "    shuffled_movies = train_movies[shuffle]\n",
    "    shuffled_ratings = train_ratings[shuffle]\n",
    "    for epoch in range(epochs):\n",
    "        for user, movie, rating in zip(shuffled_users, shuffled_movies, shuffled_ratings):\n",
    "            # update the model using the gradient at a single example\n",
    "            single_example_step(model, user, movie, rating)\n",
    "        # after each Epoch, we'll evaluate our model\n",
    "        predicted = predict(model)\n",
    "        train_loss = np.mean((train_ratings - predicted[train_users, train_movies])**2)\n",
    "        test_loss = np.mean((test_ratings - predicted[test_users, test_movies])**2)\n",
    "        print(\"Loss after epoch #{} is: train/{} --- test/{}\".format(epoch+1, train_loss, test_loss))\n",
    "\n",
    "sgd_model = initialize(n_users, n_movies, k)\n",
    "train_sgd(sgd_model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
